{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### DS7337 NLP - HW 6\n",
    " ### Neil Benson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <u>**HW 6:**</u>\n",
    "\n",
    "1.\tEvaluate text similarity of Amazon book search results by doing the following:  \n",
    "    a.\tDo a book search on Amazon. Manually copy the full book title (including subtitle) of each of the top 24 books listed in the first two pages of search results.  \n",
    "    b.\tIn Python, run one of the text-similarity measures covered in this course, e.g., cosine similarity. Compare each of the book titles, pairwise, to every other one.  \n",
    "    c.\tWhich two titles are the most similar to each other? Which are the most dissimilar? Where do they rank, among the first 24 results?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "#### Evaluate text similarity of Amazon book search results \"Data Mining\"\n",
    " Do a book search on Amazon. Manually copy the full book title (including subtitle) of each of the top 24 books listed in the first two pages of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_titles = [\n",
    "    \"Practical Data Science with R\",\n",
    "    \"Build a Career in Data Science\",\n",
    "    \"Introduction to Statistics: An Intuitive Guide for Analyzing Data and Unlocking Discoveries\",\n",
    "    \"Data Mining: Practical Machine Learning Tools and Techniques (Morgan Kaufmann Series in Data Management Systems)\",\n",
    "    \"Becoming a Data Head: How to Think, Speak and Understand Data Science, Statistics and Machine Learning\",\n",
    "    \"Machine Learning Engineering\",\n",
    "    \"Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\",\n",
    "    \"Data Science from Scratch: First Principles with Python\",\n",
    "    \"Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)\",\n",
    "    \"Introduction to Data Mining (2nd Edition) (What's New in Computer Science)\",\n",
    "    \"Introduction to Data Mining and Analytics\",\n",
    "    \"Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)\",\n",
    "    \"Data Mining: The Textbook\",\n",
    "    \"Learn Data Mining Through Excel: A Step-by-Step Approach for Understanding Machine Learning Methods\",\n",
    "    \"Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\",\n",
    "    \"R in Action: Data Analysis and Graphics with R\",\n",
    "    \"Data Analytics: Systems Engineering - Cybersecurity - Project Management\",\n",
    "    \"Teach Yourself Data Analytics in 30 Days: Learn to use Python and Jupyter Notebooks by exploring fun, real-world data projects\",\n",
    "    \"The Hundred-Page Machine Learning Book\",\n",
    "    \"Pattern Recognition and Machine Learning (Information Science and Statistics)\",\n",
    "    \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics)\",\n",
    "    \"Data Mining for Business Analytics: Concepts, Techniques and Applications in Python\",\n",
    "    \"Data Mining Techniques: For Marketing, Sales, and Customer Relationship Management\",\n",
    "    \"Data Mining and Machine Learning: Fundamental Concepts and Algorithms\",\n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(amazon_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compare each of the book titles, pairwise, to every other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  linear_kernel is equivalent to cosine_similarity\n",
    "#  because the TfidfVectorizer produces normalized vectors.\n",
    "#  Get the cosine similarities between all documents in the corpus\n",
    "cosine_similarities = linear_kernel(tfidf, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each title has a cosine similarity of 1 as it's 100% related to itself.  \n",
    "We can see this at indeces `[0][0]`, `[1][1]`, `[2][2]`, `[3][3]`, and so on.  \n",
    "\n",
    "Which two titles are the most similar to each other? Which are the most dissimilar? Where do they rank, among the first 24 results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.21253315 0.02861595 0.27378777 0.14561349 0.\n",
      "  0.21767678 0.32327429 0.         0.13782232 0.05650165 0.06606777\n",
      "  0.05981357 0.02384484 0.16335444 0.24668639 0.03830106 0.04120626\n",
      "  0.         0.1198639  0.02536112 0.03569238 0.03357407 0.03978628]\n",
      " [0.21253315 1.         0.02297581 0.12043641 0.11691338 0.\n",
      "  0.05038594 0.12110806 0.         0.18658883 0.0453653  0.13164859\n",
      "  0.04802444 0.01914507 0.13115762 0.13222069 0.03075201 0.08210881\n",
      "  0.         0.09623898 0.08070813 0.11358588 0.02695669 0.03194449]\n",
      " [0.02861595 0.02297581 1.         0.04962078 0.17533079 0.\n",
      "  0.106402   0.01630627 0.02174321 0.15739672 0.31869839 0.05424029\n",
      "  0.02792683 0.05283968 0.14495006 0.04882149 0.01788269 0.06986605\n",
      "  0.         0.13440494 0.08826462 0.10436961 0.09817538 0.0749265 ]\n",
      " [0.27378777 0.12043641 0.04962078 1.         0.17089003 0.19282715\n",
      "  0.08535214 0.03444115 0.26806659 0.1124037  0.1587134  0.73964752\n",
      "  0.12328379 0.11529761 0.12360428 0.13275817 0.24088858 0.08615091\n",
      "  0.11199782 0.15420888 0.20333803 0.23930959 0.25778244 0.25188971]\n",
      " [0.14561349 0.11691338 0.17533079 0.17089003 1.         0.17467989\n",
      "  0.0985773  0.08297514 0.19592611 0.12921267 0.22157894 0.10378146\n",
      "  0.05343422 0.0812264  0.18589351 0.09341332 0.03421611 0.09920371\n",
      "  0.10145753 0.32181886 0.14311479 0.08024791 0.07548527 0.24334905]\n",
      " [0.         0.         0.         0.19282715 0.17467989 1.\n",
      "  0.         0.         0.39182055 0.         0.         0.\n",
      "  0.         0.15214627 0.         0.         0.30664856 0.\n",
      "  0.25759633 0.23043851 0.07658412 0.         0.         0.25386353]\n",
      " [0.21767678 0.05038594 0.106402   0.08535214 0.0985773  0.\n",
      "  1.         0.21231849 0.02384143 0.032674   0.10172603 0.0932981\n",
      "  0.06124354 0.07014615 0.14906225 0.32998881 0.03921673 0.11026842\n",
      "  0.         0.05608673 0.04566036 0.22293401 0.12483765 0.10252563]\n",
      " [0.32327429 0.12110806 0.01630627 0.03444115 0.08297514 0.\n",
      "  0.21231849 1.         0.         0.07853548 0.03219642 0.03764749\n",
      "  0.03408365 0.01358754 0.09308449 0.14056965 0.02182515 0.08144696\n",
      "  0.         0.06830222 0.01445156 0.12075817 0.01913156 0.02267147]\n",
      " [0.         0.         0.02174321 0.26806659 0.19592611 0.39182055\n",
      "  0.02384143 0.         1.         0.         0.04293156 0.10680723\n",
      "  0.         0.13441577 0.01964272 0.03156938 0.         0.01565484\n",
      "  0.22757711 0.25846661 0.14965848 0.02712008 0.02551053 0.28474081]\n",
      " [0.13782232 0.18658883 0.15739672 0.1124037  0.12921267 0.\n",
      "  0.032674   0.07853548 0.         1.         0.37491282 0.12286807\n",
      "  0.09903802 0.03948177 0.25984256 0.08574174 0.01994189 0.09343165\n",
      "  0.         0.06240852 0.17434494 0.11417256 0.05559122 0.06587727]\n",
      " [0.05650165 0.0453653  0.31869839 0.1587134  0.22157894 0.\n",
      "  0.10172603 0.03219642 0.04293156 0.37491282 1.         0.17348903\n",
      "  0.17535631 0.06990626 0.24887941 0.0963971  0.20964303 0.23172794\n",
      "  0.         0.10099607 0.10981287 0.31700701 0.14537469 0.22790476]\n",
      " [0.06606777 0.13164859 0.05424029 0.73964752 0.10378146 0.\n",
      "  0.0932981  0.03764749 0.10680723 0.12286807 0.17348903 1.\n",
      "  0.29393408 0.05372285 0.13511138 0.14511746 0.26331442 0.09417124\n",
      "  0.10743419 0.0590477  0.25336073 0.3735106  0.28178104 0.27944897]\n",
      " [0.05981357 0.04802444 0.02792683 0.12328379 0.05343422 0.\n",
      "  0.06124354 0.03408365 0.         0.09903802 0.17535631 0.29393408\n",
      "  1.         0.0740039  0.13068976 0.0405475  0.03737871 0.04021394\n",
      "  0.1945282  0.         0.20091175 0.11077346 0.10419916 0.12347915]\n",
      " [0.02384484 0.01914507 0.05283968 0.11529761 0.0812264  0.15214627\n",
      "  0.07014615 0.01358754 0.13441577 0.03948177 0.06990626 0.05372285\n",
      "  0.0740039  1.         0.08977736 0.01616437 0.01490112 0.1292079\n",
      "  0.08836956 0.07905295 0.05765038 0.09618031 0.0904721  0.1363143 ]\n",
      " [0.16335444 0.13115762 0.14495006 0.12360428 0.18589351 0.\n",
      "  0.14906225 0.09308449 0.01964272 0.25984256 0.24887941 0.13511138\n",
      "  0.13068976 0.08977736 1.         0.07915452 0.04846544 0.09787777\n",
      "  0.         0.09676715 0.07163764 0.26350055 0.14788761 0.13783779]\n",
      " [0.24668639 0.13222069 0.04882149 0.13275817 0.09341332 0.\n",
      "  0.32998881 0.14056965 0.03156938 0.08574174 0.0963971  0.14511746\n",
      "  0.0405475  0.01616437 0.07915452 1.         0.02596421 0.0905093\n",
      "  0.         0.07426665 0.09421884 0.13260039 0.0572805  0.1087872 ]\n",
      " [0.03830106 0.03075201 0.01788269 0.24088858 0.03421611 0.30664856\n",
      "  0.03921673 0.02182515 0.         0.01994189 0.20964303 0.26331442\n",
      "  0.03737871 0.01490112 0.04846544 0.02596421 1.         0.08932091\n",
      "  0.         0.         0.01584868 0.13243255 0.12457281 0.02486326]\n",
      " [0.04120626 0.08210881 0.06986605 0.08615091 0.09920371 0.\n",
      "  0.11026842 0.08144696 0.01565484 0.09343165 0.23172794 0.09417124\n",
      "  0.04021394 0.1292079  0.09787777 0.0905093  0.08932091 1.\n",
      "  0.         0.03682786 0.0552473  0.19623419 0.03969093 0.06732073]\n",
      " [0.         0.         0.         0.11199782 0.10145753 0.25759633\n",
      "  0.         0.         0.22757711 0.         0.         0.10743419\n",
      "  0.1945282  0.08836956 0.         0.         0.         0.\n",
      "  1.         0.13384324 0.12696207 0.         0.         0.14744895]\n",
      " [0.1198639  0.09623898 0.13440494 0.15420888 0.32181886 0.23043851\n",
      "  0.05608673 0.06830222 0.25846661 0.06240852 0.10099607 0.0590477\n",
      "  0.         0.07905295 0.09676715 0.07426665 0.         0.03682786\n",
      "  0.13384324 1.         0.15890944 0.06379972 0.06001327 0.2741388 ]\n",
      " [0.02536112 0.08070813 0.08826462 0.20333803 0.14311479 0.07658412\n",
      "  0.04566036 0.01445156 0.14965848 0.17434494 0.10981287 0.25336073\n",
      "  0.20091175 0.05765038 0.07163764 0.09421884 0.01584868 0.0552473\n",
      "  0.12696207 0.15890944 1.         0.11313894 0.06525234 0.14613346]\n",
      " [0.03569238 0.11358588 0.10436961 0.23930959 0.08024791 0.\n",
      "  0.22293401 0.12075817 0.02712008 0.11417256 0.31700701 0.3735106\n",
      "  0.11077346 0.09618031 0.26350055 0.13260039 0.13243255 0.19623419\n",
      "  0.         0.06379972 0.11313894 1.         0.26161555 0.27876856]\n",
      " [0.03357407 0.02695669 0.09817538 0.25778244 0.07548527 0.\n",
      "  0.12483765 0.01913156 0.02551053 0.05559122 0.14537469 0.28178104\n",
      "  0.10419916 0.0904721  0.14788761 0.0572805  0.12457281 0.03969093\n",
      "  0.         0.06001327 0.06525234 0.26161555 1.         0.13542418]\n",
      " [0.03978628 0.03194449 0.0749265  0.25188971 0.24334905 0.25386353\n",
      "  0.10252563 0.02267147 0.28474081 0.06587727 0.22790476 0.27944897\n",
      "  0.12347915 0.1363143  0.13783779 0.1087872  0.02486326 0.06732073\n",
      "  0.14744895 0.2741388  0.14613346 0.27876856 0.13542418 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# flatten to get the indices with the highest similarities\n",
    "cosine_similarities_flatten = linear_kernel(tfidf, tfidf).flatten()\n",
    "print(cosine_similarities)\n",
    "\n",
    "# pairwise cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the top 2 related documents, use argsort and some negative array slicing\n",
    "`[-25:-26]` range becaus we want to ignore first 24 which are just the documents cosine similarities to themselves after sorting.\n",
    "\n",
    "### Top 2 most similar titles\n",
    "\n",
    "The index in the flattened array of the most similar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([267])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities_flatten.argsort()[-25:-26:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarities of the top 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73964752])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities_flatten[related_docs_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the flattened indeces back to `amazon_titles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_titles_indeces_related = [\n",
    "    (idx // len(amazon_titles), idx % len(amazon_titles))\n",
    "    for idx in related_docs_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `amazon_titles` indeces for the top 2 most similar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_titles_indeces_related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 2 most dissimilar titles\n",
    "\n",
    "The index in the flattened array of the most dissimilar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([354])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top two unrelated docs\n",
    "unrelated_docs_indices = cosine_similarities_flatten.argsort()[0:1]\n",
    "unrelated_docs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarities of the of the 2 most dissimilar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities_flatten[unrelated_docs_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the flattened indeces back to `amazon_titles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_titles_indeces_unrelated = [\n",
    "    (idx // len(amazon_titles), idx % len(amazon_titles))\n",
    "    for idx in unrelated_docs_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `amazon_titles` indeces for the top 2 most dissimilar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 18)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_titles_indeces_unrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we explore further, we can see that the cosine similarity pairwise matrix is simetrical with indeces of high cosine similarity in pairs such as `(11,3)` and `(3,11)` or `(5,8)` and `(8,5)` and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the two most similar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most similar docs:\n",
      "Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)\n",
      "Data Mining: Practical Machine Learning Tools and Techniques (Morgan Kaufmann Series in Data Management Systems)\n",
      "\n",
      "\n",
      "The two most similar book titles from Amazon search results were ranked 11 and 3.\n"
     ]
    }
   ],
   "source": [
    "# reviewing the two most similar titles\n",
    "for index, element in enumerate(amazon_titles_indeces_related):\n",
    "    if index % 2 == 0:\n",
    "        print(\n",
    "            f\"Two most similar docs:\\n{amazon_titles[element[0]]}\\n{amazon_titles[element[1]]}\\n\\n\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"The two most similar book titles from Amazon search results were ranked {amazon_titles_indeces_related[0][0]} and {amazon_titles_indeces_related[0][1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the two most dissimilar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most similar docs:\n",
      "Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\n",
      "The Hundred-Page Machine Learning Book\n",
      "\n",
      "\n",
      "The two most dissimilar book titles from Amazon search results were ranked 14 and 18.\n"
     ]
    }
   ],
   "source": [
    "# reviewing the two most dissimilar titles\n",
    "for index, element in enumerate(amazon_titles_indeces_unrelated):\n",
    "    if index % 2 == 0:\n",
    "        print(\n",
    "            f\"Two most similar docs:\\n{amazon_titles[element[0]]}\\n{amazon_titles[element[1]]}\\n\\n\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"The two most dissimilar book titles from Amazon search results were ranked {amazon_titles_indeces_unrelated[0][0]} and {amazon_titles_indeces_unrelated[0][1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "2.\tNow evaluate using a major search engine.  \n",
    "    a.\tEnter one of the book titles from question 1a into Google, Bing, or Yahoo!. Copy the capsule of the first organic result and the 20th organic result. Take web results only (i.e., not video results), and skip sponsored results.  \n",
    "    b.\tRun the same text similarity calculation that you used for question 1b on each of these capsules in comparison to the original query (book title).  \n",
    "    c.\tWhich one has the highest similarity measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_results_1 = [\n",
    "    \"Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)\",  # original title\n",
    "    \"\"\"Data Mining: Concepts and Techniques (The ... - Amazon\n",
    "        https://www.amazon.com › Data-Mining-Concepts-Tec...\n",
    "        Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems) [Han, Jiawei, Kamber, Micheline, Pei, Jian] on ...\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "tfidf_web_1 = TfidfVectorizer().fit_transform(web_results_1)\n",
    "cosine_similarities_web_1 = linear_kernel(tfidf_web_1, tfidf_web_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cosine similarity, original query and 1st result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.74302404],\n",
       "       [0.74302404, 1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities_web_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web result 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_results_20 = [\n",
    "    \"Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)\",  # original title\n",
    "    \"\"\"Concepts and Techniques (The Morgan Kaufmann Series in ...\n",
    "        http://www.gate2biotech.com › data-mining-concepts-a...\n",
    "        Data Mining: Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems). \n",
    "        Authors: Jiawei Han, Micheline Kamber Publishing: ...\"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "tfidf_web_20 = TfidfVectorizer().fit_transform(web_results_20)\n",
    "cosine_similarities_web_20 = linear_kernel(tfidf_web_20, tfidf_web_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cosine similarity, original query and 20th result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82299487],\n",
       "       [0.82299487, 1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities_web_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cosine similarity comparison between 1st and 20th result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities_web_20[0][1] > cosine_similarities_web_1[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The 20th result has a higher similarity to the original query than the first result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
