{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS7337 NLP - HW 3  \n",
    "## Neil Benson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Homework 3\n",
    "\n",
    " <u>**HW 3:**</u>\n",
    "\n",
    " [book link](http://www.nltk.org/book/)\n",
    "\n",
    " 1. Compare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "\n",
    "     - What is the edit distance between your nickname and your given name?\n",
    "     - What is the percentage string match between your nickname and your given name?\n",
    " Show your work for both calculations.\n",
    "\n",
    " 2. Find a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words.\n",
    "\n",
    " 3. Run one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import Levenshtein as lev\n",
    "from _cleaning_options.cleaner import simple_cleaner\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bensonnd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Q1, Pt 1 - Edit Distance of Name and Nickname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Edit distance is the number of edits between two strings that are needed in order for them to be equal. This can include additions, subsitutions, and deletions.\n",
    "\n",
    " Source: https://www.datasciencelearner.com/nltk-edit_distance-implement-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The edit distance between my name and nickname is 6.\n"
     ]
    }
   ],
   "source": [
    "name = \"Neil Benson\"\n",
    "nickname = \"Neilybob\"\n",
    "\n",
    "\n",
    "distance = nltk.edit_distance(nickname, name, transpositions=False)\n",
    "print(f\"The edit distance between my name and nickname is {distance}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Q1, Pt 2 - Percentage String Match of Name and Nickname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the Levenshtein Distance to measure how far apart two sequences of words are; the minimum amount of these operations that need to be done to name in order to turn it into nickname, correspond to the Levenshtein distance between those two strings.\n",
    "\n",
    " ![title](https://miro.medium.com/max/875/0*kWblkNhdDJ7XWthC.jpg)\n",
    "\n",
    " Where i and j are indexes to the last character of the substring we’ll be comparing. The second term in the last expression is equal to 1 if those characters are different, and 0 if they’re the same.\n",
    "\n",
    " Source: https://towardsdatascience.com/fuzzywuzzy-how-to-measure-string-distance-on-python-4e8852d7c18f\n",
    "\n",
    "\n",
    "\n",
    " Thankfully there are packages for this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.16% string match between name and nickname.\n"
     ]
    }
   ],
   "source": [
    "ratio = lev.ratio(name.lower(), nickname.lower())\n",
    "print(f\"{ratio*100:.2f}% string match between name and nickname.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Q2 - Exclude stopwords to guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text\n",
    "# The Great Gatsby\n",
    "book_text_url = \"https://www.gutenberg.org/files/64317/64317-0.txt\"\n",
    "\n",
    "\n",
    "# get the text as raw string format\n",
    "def text_getter(target_url):\n",
    "    \"\"\"\n",
    "    Retrieves book text from a url and returns decoded text for nl analysis\n",
    "    param:\n",
    "        target_url (str): the url to retrieve the book text from\n",
    "    return:\n",
    "        decoded_text: the book text decoded\n",
    "    \"\"\"\n",
    "    response = requests.get(target_url)\n",
    "    decoded_text = response.text\n",
    "\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "# replace several substrings at once\n",
    "def replace_multiple(mainString, toBeReplaces, newString):\n",
    "    \"\"\"\n",
    "    Replace a set of multiple sub strings with a new string in main string.\n",
    "    \"\"\"\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces:\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString:\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "\n",
    "    return mainString\n",
    "\n",
    "\n",
    "# retrieving raw text from book\n",
    "raw_text = text_getter(book_text_url)\n",
    "\n",
    "text_no_headers = simple_cleaner(raw_text)\n",
    "\n",
    "# strip out title and table of contents\n",
    "regex = r\"(^.*\\n\\s+I\\n\\n)(.*)\"\n",
    "book_text = re.search(regex, text_no_headers)\n",
    "\n",
    "# first two sentences - regex not working as expected\n",
    "first_two_sent = \"\\n\\nIn my younger and more vulnerable years my father gave me some advice\\nthat I\\x80\\x99ve been turning over in my mind ever since.\\n\\n\\x80\\x9cWhenever you feel like criticizing anyone,\\x80\\x9d he told me, \\x80\\x9cjust\\nremember that all the people in this world haven\\x80\\x99t had the advantages\\nthat you\\x80\\x99ve had.\\x80\\x9d\\n\\n\"\n",
    "\n",
    "# cleaning up first 2 sentences\n",
    "text = \" \".join(replace_multiple(first_two_sent.lower(), [\"\\n\", \"\\r\",], \" \").split())\n",
    "\n",
    "# sentence tokenizer\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "# removing punctuation\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "for i, sentence in enumerate(sentence_tokens):\n",
    "    sentence_tokens[i] = tokenizer.tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out stop words from each of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger vulnerable years father gave advice turning mind ever since\n",
      "whenever feel like criticizing anyone told remember people world advantages\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# # filtering out stopwords from text\n",
    "stop_words_rem = []\n",
    "for sentence in sentence_tokens:\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "            new_sentence.append(word)\n",
    "    stop_words_rem.append(new_sentence)\n",
    "\n",
    "# reviewing the sentences after removing the stop words\n",
    "for sentence in stop_words_rem:\n",
    "    print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " They were not able to guess the book from which these sentences originated. First, when listening to the sentence\n",
    " without stop words, they couldn't grasp the concept of putting together a full sentence. They sort of followed along\n",
    " but it was difficult.\n",
    "\n",
    " Second, when I read the sentences in full, they still didn't recognize the book. I confirmed, and it rang a bell,\n",
    " but these first two lines were not the most memorable from the book.\n",
    "\n",
    " Stop words are important. But also, I can see how they can really affect the outcome of NLP efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Q3 - Percentage Match of text using Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed sentence:\n",
      "\n",
      "  younger vulner year father gave advic turn mind ever sinc\n",
      "\n",
      "  64.77% of stem words are morphological roots when calculating Levenshtein Distance ratio.\n",
      "\n",
      "Stemmed sentence:\n",
      "\n",
      "  whenev feel like critic anyon told rememb peopl world advantag\n",
      "\n",
      "  60.49% of stem words are morphological roots when calculating Levenshtein Distance ratio.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def stemmer(text):\n",
    "    stems = [ps.stem(word) for word in text]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "\n",
    "sentence_tokens_txt = [\" \".join(sentence) for sentence in sentence_tokens]\n",
    "stem_sentences_txt = [stemmer(sentence) for sentence in stop_words_rem]\n",
    "\n",
    "zip_stem_orig_sent = zip(sentence_tokens_txt, stem_sentences_txt)\n",
    "\n",
    "for sentence in zip(zip_stem_orig_sent):\n",
    "    ratio_orig = lev.ratio(sentence[0][0], sentence[0][1])\n",
    "    print(\n",
    "        f\"Stemmed sentence:\\n\\n  {sentence[0][1]}\\n\\n  {ratio_orig*100:.2f}% of stem words are morphological roots when calculating Levenshtein Distance ratio.\\n\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
