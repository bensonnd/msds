---
title: "Case Study 2 - Talent Attrition and Income"
author: "By Neil Benson"
date: "08/05/2020"
knit: (function(inputFile, encoding) { 
      out_dir <- 'C:/Git/MSDS_6306_Doing-Data-Science/nbenson_unit14_casestudy2/';
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), out_dir, 'Case-Study-2-Talent.html')) })
output: html_document 
---
# A review and analysis of beer and breweries by state. We will review missing data and how we filled it, number of breweries by state, summary statistics like medians and means of ABV and IBU at the beer level, and finally a review of the relationship between Ales, IPAs and other types of beer. 


## Importing the necessary libraries
```{r,warning=FALSE,message=FALSE}
library(ggthemes)
library(dplyr) 
library(tidyverse)
library(forcats)
library(naniar)
library(corrplot)
library(imputeTS)
library(e1071)
library(caret)
library(rpart)
library(formula.tools)
library(modelr)
library(psych)
library(gridExtra)
library(ggplot2)
library(grid)
library(stringr)
```


## Importing and cleaning the data
As there weren't any missing data, nor anything out of the ordinary, we did not cleanse or impute data. 
```{r,fig.height=6, fig.width=9, warning=FALSE,message=FALSE}
########### Loading the data
talent <- read.csv("C:/Git/MSDS_6306_Doing-Data-Science/nbenson_unit14_casestudy2/data/CaseStudy2-data.csv")

# reviewing the structure of the data
str(talent)


# drop unneeded columns
drop.cols <- c("EmployeeCount","EmployeeNumber","Over18","StandardHours")
talentcleanedDF <- talent %>% select(-one_of(drop.cols))


# checking for missing values in the data
vis_miss(talentcleanedDF) + xlab("Data Columns")
```


## Overall Profile of Attritioned Employees
Profile - lower monthly income, younger, further from home, with fewer total working years, fewer years at company, fewer years in current role, slightly fewer years with current manager, and fewer stock options.

Profile of Attrition:  
  
  * Median Age: 32  
  * Department (most common): Research & Development  
  * Education Field (most common): Life Sciences  
  * Gender (most common): Men  
  * Job Role (most common): Sales Executive  
  * Marital Status (most common): Single  
  * Median Years at Company: 3  
  * Median Total Working Years: 6.5  
  * Median Monthly Income: $3,171  

```{r,warning=FALSE,message=FALSE}

print(talentcleanedDF %>% count(talentcleanedDF$Attrition))

# group similar columns together
colstomove <- c("MonthlyIncome", "MonthlyRate", "DailyRate", "HourlyRate")
talentcleanedDF <- talentcleanedDF %>% select(-colstomove,all_of(colstomove))


# compare like or similar column types to reduce number of columns
talentcleanedDF$monthlyhours <- talentcleanedDF$MonthlyIncome/talentcleanedDF$MonthlyRate


# split the dataframe into those who attritioned and those who did not to create a general overal profile of the two
attrittioned <- talentcleanedDF %>% filter(Attrition == "Yes")
nonattrittioned <- talentcleanedDF %>% filter(Attrition == "No")


# overall profile of attrition vs nonattritrion employees
profile <- function (dataframe) {
  # this function returns the median value for any numeric column and the mode (or most occuring) for the chr/str columns 
  
  dfnames <- colnames(dataframe)
  outputdf <- data.frame()[1,]
  
  for(name in dfnames){

    if (class(dataframe[ , name ]) == 'integer' | class(dataframe[ , name ]) == 'numeric') 
      { 
        outputdf[ , sprintf("%s_median", name) ] <- median(dataframe[ , name])
      } 
    else
      { 
        # get the most occuring from a chr column, similar to numerical mode
        x <- c(dataframe[ , name])
        tt <- table(x)
        outputdf[ , sprintf("%s_mode", name) ] <- names(tt[tt==max(tt)])
      }
  }
 outputdf[ , "Profile"] <- deparse(substitute(dataframe))
 return(outputdf)
}


# get an overall profile for those who have attritione and those who have not
profiledf <- rbind(profile(attrittioned), profile(nonattrittioned))
profiledf <- profiledf %>% select("Profile", everything())
```


# Numerical Explanatory Variables EDA
There is a degree of correlation between Total Working Years and Monthly Income of .78, and Total Working Years and Age, however we will continue to explore these variables. Most of the numerical explanatory variables have a right skew, and are not overly linearly related. 

We do see that TotaWorkingYears, PercentSalaryHike, YearsAtCompany, YearsInCurrentRoll, YearsSinceLastPromotion, MonthlyIncome, monthlyhours are heavily right skewed. We log2 transform these variables for our analysis. We tested our models with a log2 transformation on these variables, but did not see any increase in model performance, and abandoned the transformations.

When looking at the overlap of density, monthly hours, training time last year, and years since last promotion do not seem to uniquely identify individuals who might attrition, however may still be used to identify individuals in the model. More exploration is needed.

```{r, fig.height=9, fig.width=9}
# sorting the columns so that Atrrition column is first
talentcleanedDF <- talentcleanedDF %>% select("ID", "Attrition", everything())


corvars = c("Age", "DistanceFromHome", "NumCompaniesWorked", "PercentSalaryHike", "TotalWorkingYears", "TrainingTimesLastYear", "YearsAtCompany", "YearsInCurrentRole", "YearsSinceLastPromotion", "YearsWithCurrManager", "MonthlyIncome", "MonthlyRate", "monthlyhours", "DailyRate", "HourlyRate")


# check for correlation of numerical independent variables
pairs.panels(talentcleanedDF[,corvars])

# transformin heavily right skewed variables

# colstolog <- c("TotalWorkingYears", "PercentSalaryHike", "YearsAtCompany", "YearsInCurrentRole", "YearsSinceLastPromotion", "MonthlyIncome", "monthlyhours", "DistanceFromHome")
# 
# for (col in colstolog)
# {
#   talentcleanedDF[col] <- log(talentcleanedDF[col] + 1,2)
# }
```
```{r, fig.height=6, fig.width=9}
# reviewing the density of each numerical variable by attrition
talent_reshaped <- data.frame(Attrition = talentcleanedDF$Attrition,independent.variable = c(talentcleanedDF$Age,talentcleanedDF$DistanceFromHome,talentcleanedDF$NumCompaniesWorked,talentcleanedDF$PercentSalaryHike, talentcleanedDF$TotalWorkingYears, talentcleanedDF$TrainingTimesLastYear, talentcleanedDF$YearsAtCompany, talentcleanedDF$YearsInCurrentRole, talentcleanedDF$YearsSinceLastPromotion, talentcleanedDF$YearsWithCurrManager, talentcleanedDF$MonthlyIncome, talentcleanedDF$MonthlyRate, talentcleanedDF$monthlyhours, talentcleanedDF$DailyRate, talentcleanedDF$HourlyRate),
Variable.name = rep(corvars,each=nrow(talentcleanedDF)))

ggplot(talent_reshaped,aes(independent.variable,fill=Attrition)) + 
geom_density(alpha=.7, color='black') + facet_wrap(~Variable.name,scales="free", ncol = 5) + 
xlab("") + theme(legend.position='bottom') + theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(), axis.ticks.y=element_blank()) + ggtitle("Density by Independent Numerical Variables")

```

# Factor Analysis:
The top three most influential predictor factors in accordance with an F-test are:  

* OverTime  
    * pvalue: < 2e-16  
    * f statistic: 79.419  

* JobInvolvement  
    * pvalue: < 2.36e-08  
    * f statistic: 31.773  
    
* MaritalStatus  
    * pvalue: < 1.27e-05  
    * f statistic: 19.285  
    
```{r}
# convert categorical variables to factors
names <- c("WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "PerformanceRating", "OverTime", "MaritalStatus", "JobSatisfaction", "JobLevel", "JobInvolvement", "Gender", "EnvironmentSatisfaction", "BusinessTravel", "Attrition", "Education", "Department", "EducationField", "JobRole")


# set the categorical columns to factor
for (name in names)
{
  talentcleanedDF[,name]<-factor(talentcleanedDF[,name])
}

# convert factors to numeric for factor analysis
talentcleanedASNUMDF <- talentcleanedDF[,names] %>% mutate_all(as.numeric)

# adding suffix ASNUM to numerical representation of factors columns
colnames(talentcleanedASNUMDF) <- paste(colnames(talentcleanedASNUMDF), "ASNUM", sep = "_")

# adding numerical factor columns to base Df in order to perform stepwise for variable selection
talentcleanedwnumDF <- cbind(talentcleanedDF, talentcleanedASNUMDF)

# dropping factor columns
talentcleanedwnumDF <- talentcleanedwnumDF[, sapply(talentcleanedwnumDF, class) != "factor"]

#  columns for anova factor analysis
anovacols <- c(colnames(talentcleanedASNUMDF))
anovacols <- anovacols[anovacols != "Attrition_ASNUM"]

#  creating the formula to pass into aov function
anovafmla <- as.formula(paste("Attrition_ASNUM ~ ", paste(anovacols, collapse= "+")))

anova_fit <- aov(anovafmla, data=talentcleanedwnumDF)

summary(anova_fit)
```


## Modeling and Optimization
Variable selection using forward, backward, and stepwise. We have converted all factorial columns to numerical to proceed.  

Upon initial review of variable impacts in the model, we have determined that at a minium we would like to include the following (as this selection is preliminary, these are most likely to change):  

* DistanceFromHome  
* EnvironmentSatisfaction  
* JobInvolvement  
* JobSatisfaction  
* MaritalStatus  
* NumCompaniesWorked  
* OverTime  
* StockOptionLevel  
* YearsSinceLastPromotion  

Additionally, we would like to further investigate the following variables:  

* Age  
* Department  
* JobLevel  
* RelationshipSatisfaction  
* WorkLifeBalance  
* YearsInCurrentRole  
* YearsWithCurrManager  
* DailyRate  
* MonthlyIncome  
* HourlyRate  
* PercentSalaryHike  
* MonthlyRate  
* monthlyhours  

And we will premilinarily be moving forward without the following variables, as they statistically ignisificant:  

* PerformanceRating  
* BusinessTravel  
* Education  
* EducationField  
* Gender  
* JobRole  
* TotalWorkingYears  
* TrainingTimesLastYear  
* YearsAtCompany  


```{r,warning=FALSE,message=FALSE}

# creating my own not in operator
`%notin%` <- Negate(`%in%`)

# remove ID and attrition from col list to build formula
#  columns for anova factor analysis
lmcols <- c(colnames(talentcleanedwnumDF))
lmcols <- lmcols[lmcols %notin% c("Attrition_ASNUM","ID")]
lmfmla <- as.formula(paste("Attrition_ASNUM ~ ", paste(lmcols, collapse= "+")))

#define intercept-only model
intercept_only_model <- lm(Attrition_ASNUM ~ 1, data = talentcleanedwnumDF)

#define total model
total_model <- lm(lmfmla, data = talentcleanedwnumDF)

# set the different models
talent_back_model <- step(total_model, 
                          direction = "backward", trace=FALSE)

talent_step_model <- step(intercept_only_model, 
                          direction = "both", scope = formula(total_model), trace=FALSE)

talent_fwd_model <- step(intercept_only_model, 
                         direction = "forward", scope = formula(total_model), trace=FALSE)

summary(talent_back_model)
summary(talent_step_model)
summary(talent_fwd_model)
```


# Further Refining the Model
```{r,warning=FALSE,message=FALSE}

# remove ID and attrition from col list to build formula columns for anova factor analysis
mincols <- c("DistanceFromHome","EnvironmentSatisfaction_ASNUM","JobInvolvement_ASNUM","JobSatisfaction_ASNUM","MaritalStatus_ASNUM","NumCompaniesWorked","OverTime_ASNUM","StockOptionLevel_ASNUM","YearsSinceLastPromotion")

mincols <- mincols[mincols %notin% c("Attrition_ASNUM","ID")]
minfmla <- as.formula(paste("Attrition_ASNUM ~ ", paste(mincols, collapse= "+")))

#define minimum model
min_model <- lm(minfmla, data = talentcleanedwnumDF)

# additional columns to review
addcols <- c("Age","Department_ASNUM","JobLevel_ASNUM","RelationshipSatisfaction_ASNUM","WorkLifeBalance_ASNUM","YearsInCurrentRole","YearsWithCurrManager","DailyRate","MonthlyIncome","HourlyRate","PercentSalaryHike","MonthlyRate","monthlyhours")

allcols <- c(mincols, addcols)
allfmla <- as.formula(paste("Attrition_ASNUM ~ ", paste(allcols, collapse= "+")))

#define total model
total_model <- lm(allfmla, data = talentcleanedwnumDF)

# set the different models
talent_back_new_model <- step(total_model, 
                          direction = "backward", scope = c(min_model,total_model), trace=FALSE)

talent_step_new_model <- step(min_model, 
                          direction = "both", scope = c(min_model,total_model), trace=FALSE)

talent_fwd_new_model <- step(min_model, 
                         direction = "forward", scope = c(min_model,total_model), trace=FALSE)

summary(talent_back_new_model)
summary(talent_step_new_model)
summary(talent_fwd_new_model)
```


# Quick Model Check
The models we have checked that are a result forward, backward, and stepwise variable selection have not met the minimum requirements of at least 60% sensitivity and specificity. We will return our list of predictor variables that are of significance. We have created a function to train and test the data. 

Here we are cross validating our models on a 70/30 train/test split. The function trains each model on attrition based on the predictor variables we pass it, and then tests it. It returns the accuracy, sensitivity, and specificity of each model. We will use this function to evaulate all our variations of models for NB classification.  
```{r}
fwd_fmla_cols <- as.vector(strsplit(gsub("_ASNUM", "", Reduce(paste, deparse(talent_fwd_new_model[["terms"]][[3]]))), " +")[[1]])
step_fmla_cols <- as.vector(strsplit(gsub("_ASNUM", "", Reduce(paste, deparse(talent_step_new_model[["terms"]][[3]]))), " +")[[1]])
back_fmla_cols <- as.vector(strsplit(gsub("_ASNUM", "", Reduce(paste, deparse(talent_fwd_new_model[["terms"]][[3]]))), " +")[[1]])

fwd_fmla_cols <- c(fwd_fmla_cols[fwd_fmla_cols %notin% c("+")])
step_fmla_cols <- c(step_fmla_cols[step_fmla_cols %notin% c("+")])
back_fmla_cols <- c(back_fmla_cols[back_fmla_cols %notin% c("+")])

fwdflma <- as.formula(paste("Attrition ~ ", paste(fwd_fmla_cols, collapse= "+")))
stpflma <- as.formula(paste("Attrition ~ ", paste(step_fmla_cols, collapse= "+")))
bwdflma <- as.formula(paste("Attrition ~ ", paste(back_fmla_cols, collapse= "+")))


# function to train and then test the model given variables in a dataframe. Returns accuracy, sensitivity, and specificity of the test data set
modelOptimization <- function(fmla, dataframe, cols){
      # this function splits the dataframe, trains the model on the training set,
      # and runs the training model on the test. It returns the accuracy, sensitivity,
      # and specificity for each model as a named list 
  
      # convert attrition to binary
      dataframe$Attrition[dataframe$Attrition == 1] <- "Yes"
      dataframe$Attrition[dataframe$Attrition == 0] <- "No"

  
      set.seed(43)
  
      # split the date into training and test data
      trainIndices = sample(seq(1:length(dataframe[,1])),round(.7*length(dataframe[,1])))
      talent_train = dataframe[trainIndices,]
      talent_test = dataframe[-trainIndices,]
      
      # training the model
      talent_test_model = naiveBayes(fmla,data = talent_train)
      
      # testing the model's prediction
      test_pred_val <- as.data.frame(predict(talent_test_model,talent_test[, cols, drop=FALSE],type="raw"))
      talent_test$pred_no <- test_pred_val$No
      talent_test$pred_yes <- test_pred_val$Yes
      talent_test$pred_attrition = ifelse(talent_test$pred_yes > talent_test$pred_no,"Yes","No")
      
      # create table for the confusion matrix
      cmtable <- table(talent_test$pred_attrition,talent_test$Attrition)
      
      # if there are missing rows (if the model has only predicted all yes or all no)
      # then append yes or no row of 0's
      if(nrow(cmtable) < 2) {
        if ("Yes" %in% rownames(cmtable))
          {
            cmtable <- as.table(rbind(cmtable, No=as.integer(c(0, 0))))

          }
        else
          {
            cmtable <- as.table(rbind(cmtable, Yes=as.integer(c(0, 0))))
          }
      }
      
      CM = confusionMatrix(cmtable)
      
      returnlist <- c(CM$overall["Accuracy"], CM$byClass["Sensitivity"], CM$byClass["Specificity"])
      return(list(returnlist))
      
}
```
```{r, echo=FALSE}
print("fwdflma -- ")
modelOptimization(fwdflma, talentcleanedDF, fwd_fmla_cols)

print("stpflma --")
modelOptimization(stpflma, talentcleanedDF, step_fmla_cols)


print("bwdflma -- ")
modelOptimization(bwdflma, talentcleanedDF, back_fmla_cols)


# testing out a fuller model
allnewcols <- gsub("_ASNUM", "", allcols)
allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))


print("allnewfmla -- ")
modelOptimization(allnewfmla, talentcleanedDF, allnewcols)
```


# Looping Through Remaining Combinations of Variables for NB Classification
There's a lot of back and forth in here, but essentially I am testing out different combinations of variables I removed and adding them back to increase specificty and sensitivity.
```{r}
# still more to go, looks like we need to add in columns we deemed insignificant
missingcols <- c("PerformanceRating","BusinessTravel","Education","EducationField","Gender","JobRole","TotalWorkingYears","TrainingTimesLastYear","YearsAtCompany")



colstokeep <- c()
for (col in missingcols)
{

  # add one column at a time
  allnewcols <- c(allnewcols,col)
  allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))
  # print(allnewfmla)
  returnval <- modelOptimization(allnewfmla, talentcleanedDF, allnewcols)
  if (returnval[[1]][["Sensitivity"]] >= .6 && returnval[[1]][["Specificity"]] >= .5)
  {

    colstokeep <- c(colstokeep,col)

  }
  allnewcols <- c(allnewcols[allnewcols %notin% c(col)])
}


allnewcols <- c(allnewcols,colstokeep)
allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))

modelOptimization(allnewfmla, talentcleanedDF, allnewcols)

for (col in colstokeep)
{
  allnewcols <- c(allnewcols[allnewcols %notin% c(col)])
  allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))
  returnval <- modelOptimization(allnewfmla, talentcleanedDF, allnewcols)
}

# looks like gender and business travel have no bearing on the outcome of the model. We will remove it totally. Education brings down the accuracy, sensitivity, and specificity. Removing education. EducationField decreases specificity, increases sensitivity, and doesnt affect accuracy. We will keep in lieu of increased true positives.

allnewcols <- c(allnewcols,colstokeep[colstokeep %notin% c("Gender","BusinessTravel","Education")])
allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))
modelOptimization(allnewfmla, talentcleanedDF, allnewcols)


lastcheckcols <- c("PerformanceRating","BusinessTravel","EducationField")
colstokeep <- c()
for (col in lastcheckcols)
{

  # add one column at a time
  allnewcols <- c(allnewcols,col)
  allnewfmla <- as.formula(paste("Attrition ~ ", paste(allnewcols, collapse= "+")))
  # print(allnewfmla)
  returnval <- modelOptimization(allnewfmla, talentcleanedDF, allnewcols)
  if (returnval[[1]][["Sensitivity"]] >= .6 && returnval[[1]][["Specificity"]] >= .6)
  {

    colstokeep <- c(colstokeep,col)
  }
  allnewcols <- c(allnewcols[allnewcols %notin% c(col)])
}

# final model
finalcols <- c(allnewcols,lastcheckcols)
finalfmla <- as.formula(paste("Attrition ~ ", paste(finalcols, collapse= "+")))
```
```{r, echo=FALSE}

print("Final NB Attrition Model -- ")
finalfmla


print("finalfmla -- ")
modelOptimization(finalfmla, talentcleanedDF, finalcols)
```